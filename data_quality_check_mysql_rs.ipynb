{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7407231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import csv\n",
    "import sqlalchemy\n",
    "from sqlalchemy.engine import url as sa_url\n",
    "import pprint\n",
    "import datacompy as dcp\n",
    "import numpy as np\n",
    "import pprint\n",
    "from pandas.api.types import is_datetime64_ns_dtype, is_numeric_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cad987",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_password_rs='' # input password for fi_etl\n",
    "sql_password_mysql='' # input password for fi_etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf39b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connect_url_rs = sa_url.URL(drivername='postgresql',\n",
    "                                 username='fi_etl',\n",
    "                                 password=sql_password_rs,\n",
    "                                 host='bi-redshift.clfukyskh3cy.eu-west-1.redshift.amazonaws.com',\n",
    "                                 port=5439,\n",
    "                                 database='bi_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59517e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connect_url_mysql = sa_url.URL(drivername='mysql+pymysql',\n",
    "                                 username='fi_etl',\n",
    "                                 password=sql_password_mysql,\n",
    "                                 host='aws-eu-bi-rds-mysql-prod.c4kjlqvxj0sw.eu-west-1.rds.amazonaws.com',\n",
    "                                 port=21599,\n",
    "                                 database='wkda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ec4d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_conn_mysql = sqlalchemy.create_engine(db_connect_url_mysql).connect()\n",
    "db_conn_rs = sqlalchemy.create_engine(db_connect_url_rs).connect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41720941",
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql=\"select * from dwh_load.claims_weekly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77077ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs=\"select * from dwh_fi.claims_weekly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1943c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = pd.read_sql_query(sql=rs, con=db_conn_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee2fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "mysql = pd.read_sql_query(sql=mysql, con=db_conn_mysql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea58683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use datacompy for comparison chek \n",
    "# join columns parameter given on the most granular id of the datasets\n",
    "compare = dcp.Compare(\n",
    "    rs,\n",
    "    mysql,\n",
    "    join_columns=['claim_number','claim_reason_id', 'claim_evaluation_id', 'decision_id'],  #You can also specify a list of columns\n",
    "    abs_tol=0, #Optional, defaults to 0\n",
    "    rel_tol=0, #ptional, defaults to 0\n",
    "    df1_name='df_rs', #Optional, defaults to 'df1'\n",
    "    df2_name='df_mysql' #Optional, defaults to 'df2'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aabfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "In below results check : \n",
    "    It must be Any duplicates on match values: No , otherwise check again your join columns \n",
    "    Number of rows with some compared columns unequal: 779,923\n",
    "    Number of rows with all compared columns equal: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e36d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare.report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae39909",
   "metadata": {},
   "source": [
    "From the results of compare.report() above we see that all mutual rows are unequal 779923 rows \n",
    "timestamp columns are not equal because of some digits after seconds part in redshift  \n",
    "and float columns are unequal because of digits also \n",
    "Use method below to correct those and re-run datacompy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31051eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def pandas_manipulate(data):\n",
    "        \"\"\"\n",
    "        This method will remove extra digits after seconds from columns in the pandas dataframe that have 'date' or '_on' in them\n",
    "        so it will convert value 2021-07-26 09:29:04+00:00 to 2021-07-26 09:29:04\n",
    "        and also will convert float64 data type columns into integer columns , to discard decimals\n",
    "        :param data: Dataframe object to be manipulated\n",
    "        :return: the changed pandas DataFrame object\n",
    "        \"\"\"\n",
    "        # find date columns by string searching column names containing '_on' and 'date'\n",
    "        time_cols = [col for col in data.columns if '_on' in col]\n",
    "\n",
    "        # find float64 type columns\n",
    "        float_cols = list(data.select_dtypes(include=[np.float64]).columns)\n",
    "\n",
    "        # iterate over date selected columns of the pd dataframe to remove digits after seconds\n",
    "        for col in data[time_cols]:\n",
    "            data[col] = data[col].astype('datetime64[s]')\n",
    "\n",
    "        # iterate over float type selected columns of the pd dataframe to convert float64 into int\n",
    "        for col in data[float_cols]:\n",
    "            # fill na values from amount columns with a preset big integer like 1999999\n",
    "            data[col] = data[col].fillna(1999999)\n",
    "            # convert those amount columns into integers\n",
    "            data[col] = data[col].astype(int)\n",
    "            # convert back to na the rows that got the preset 1999999 value\n",
    "            data.loc[data[col] == 1999999, col] = np.nan\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f65eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs1=pandas_manipulate(rs)\n",
    "mysql1=pandas_manipulate(mysql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23418e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rs1=rs1[['claim_number','claim_reason_id', 'claim_evaluation_id', 'decision_id', 'reason_created_on']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c4d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mysql1=mysql1[['claim_number','claim_reason_id', 'claim_evaluation_id', 'decision_id', 'reason_created_on']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d15ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new datacompy run with altered dataframes \n",
    "compare = dcp.Compare(\n",
    "    df_rs1,\n",
    "    df_mysql1,\n",
    "    join_columns=['claim_number','claim_reason_id', 'claim_evaluation_id', 'decision_id'],  #You can also specify a list of columns\n",
    "    abs_tol=0, #Optional, defaults to 0\n",
    "    rel_tol=0, #ptional, defaults to 0\n",
    "    df1_name='df_rs', #Optional, defaults to 'df1'\n",
    "    df2_name='df_mysql' #Optional, defaults to 'df2'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6932397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare.report())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93355c36",
   "metadata": {},
   "source": [
    "From above we see that we still have all rows as unequal\n",
    "However this does not reflect the truth. If only 1 column exists with all values unequal \n",
    "then above check will show that datasets are unequal on all rows . It is enough for 1 column to be unequal in all rows\n",
    "for a row to be considered unequal in all columns. \n",
    "This can be tackled if we compare subsets of the original datasets \n",
    "on join columns and on columns of interest of our choice . Those columns of interest will not be checked altogether in 1 check \n",
    "but will be checked one by one, meaning that if we specify 3 columns of interest , method below will subset the initial dataset 1 into 3 subests , each having one of those columns. Then equivalent subsets from dataset 2 will be created and compared against subsets of dataset 1 .\n",
    "This way we can know if a column is indeed equal to the other , without the result being affected by other columns' inequality.\n",
    "\n",
    "Timestamp columns might not be of interest, because we know that MySQL and Redshift have a replication lag of at least 3 hours \n",
    "Also we do not care if 2 timestamps differ some minutes between them ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706994c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to iterate over the elements of a list 2 by 2 \n",
    "# test with lst=[1,2,3,4] by calling grouped(lst,2)\n",
    "def grouped(iterable, n):\n",
    "    \"s -> (s0,s1,s2,...sn-1), (sn,sn+1,sn+2,...s2n-1), (s2n,s2n+1,s2n+2,...s3n-1), ...\"\n",
    "    return zip(*[iter(iterable)]*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08232741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_one_by_one_columns(df_rs, df_mysql, columns: list, join_columns: list): \n",
    "    \"\"\"\n",
    "    This method will compare 2 subsets of 2 initial datasets, based on the column names specified \n",
    "    in columns list object . Comparison is performed on each column independently of other columns in list.\n",
    "    For example passing parameter columns=['col1', 'col2', 'col3'] ,the subset datasets compared in datacompy \n",
    "    from the initial datasets contain columns \n",
    "    ['join_column_1', 'join_column_2', ... 'join_column_n', 'col1'], ['join_column_1', 'join_column_2', ... 'join_column_n', 'col2']  etc etc. \n",
    "    :param df_rs: the Redshift pandas dataframe , or any dataset \n",
    "    :param df_mysql: the MySQL pandas dataframe , or any dataset \n",
    "    :param columns: the list of columns upon which the equality checks will be performed \n",
    "    :param join_columns: the list of columns to be used in join_columns parameter of datacompy.Compare object\n",
    "    :return: dict :dictionary object storing key = elements in columns list , value = number of matching rows for that column from the comparison\n",
    "             comp : list object storing the compare objects \n",
    "    \"\"\"\n",
    "    # list to contain pandas dataframe objects from slicing intitial dataframes \n",
    "    # from redshift and mysql with the 1 column to be checked for equality and the join columns \n",
    "    dfs=[]\n",
    "    for col in columns: \n",
    "        colm=join_columns + list(col.split(\" \"))\n",
    "        dfs.append(df_rs.loc[:,colm])\n",
    "        dfs.append(df_mysql.loc[:,colm])\n",
    "    # list to contain compare objects from comparing 2 by 2 the pandas dataframes stored in dfs\n",
    "    comp=[]\n",
    "    for x, y in grouped(dfs, 2):\n",
    "        compare = dcp.Compare(\n",
    "        x,\n",
    "        y,\n",
    "        join_columns=join_columns,  #You can also specify a list of columns\n",
    "        abs_tol=0, #Optional, defaults to 0\n",
    "        rel_tol=0, #ptional, defaults to 0\n",
    "        df1_name='df_rs', #Optional, defaults to 'df1'\n",
    "        df2_name='df_mysql' #Optional, defaults to 'df2'\n",
    "        )\n",
    "        comp.append(compare)\n",
    "    dict={}\n",
    "    for (a,b) in zip(columns,comp): \n",
    "        dict[a]=b.count_matching_rows()\n",
    "    return  dict,comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad4120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the method above \n",
    "dict,comp =comparison_one_by_one_columns(rs1, mysql1, ['stock_number','vin', 'merchant_id','company','merchant_country', 'merchant_score', 'merchant_score_category', 'account_score_category', 'claim_status', 'claim_status_id', 'fault_by', 'evaluation_amount', 'crd_credit', 'crd_auc_vouchers', 'crd_partial_refund', 'crd_full_refund', 'crd_no_comp', 'crd_free_transport', 'type', 'transport_comp', 'crd_vouchers','crd_vouchers_value', 'category_desc', 'sub_category_desc', 'part_desc', 'dmg_desc', 'branch_name', 'branch_id','sell_price', 'buy_price', 'purchase_country', 'car_mileage', 'built_year', 'evaluator_name', 'claim_created_on', 'reason_created_on'], ['claim_number','claim_reason_id', 'claim_evaluation_id', 'decision_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2aa147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print method results \n",
    "for k, v in dict.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bc59d2",
   "metadata": {},
   "source": [
    "From above we see that the vast majority of columns in the 2 datasets are 100 % equal, \n",
    "the results from the first 2 checks showed that all mutual rows were unequal because potentially one of the timestamp columns \n",
    "was not matching at all , however all other columns are matching 100% \n",
    "Find out which of the timestamp columns were causing this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1f0a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict,comp =comparison_one_by_one_columns(rs1, mysql1, ['claim_created_on','claim_last_modified_on','reason_created_on', 'first_registration_date', 'dwh_created_on', 'dwh_modified_on'], ['claim_number','claim_reason_id', 'claim_evaluation_id', 'decision_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151dee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print method results \n",
    "for k, v in dict.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6064ff",
   "metadata": {},
   "source": [
    "So 2 columns are causing all this. dwh_created_on, dwh_modified_on mismatch 100% \n",
    "Those columns have 1 distinct value , and the reason those are different is beacuse of the replication lag between mysql and redshift. \n",
    "Run below to see in more detail the compare report of one of the elements in comp list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed790ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comp[3].report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd342294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chack ALL columns now to finish the analysis\n",
    "dict,comp =comparison_one_by_one_columns(rs1, mysql1, ['built_year','sell_price','crd_vouchers_value','crd_vouchers','transport_comp','type','crd_free_transport','crd_no_comp','crd_full_refund','crd_partial_refund','crd_auc_vouchers','crd_credit','evaluation_amount','claim_status_id','dwh_report_desc','dwh_report_id','evaluator_name','purchase_country','buy_price','branch_name','dmg_desc','part_desc','sub_category_desc','category_desc','fault_by','claim_status','account_score_category','merchant_score_category','merchant_country','company','vin','stock_number','id','first_registration_date','car_mileage','branch_id','merchant_id','merchant_score','dwh_modified_on','dwh_created_on','reason_created_on','claim_last_modified_on','claim_created_on'], ['claim_number','claim_reason_id', 'claim_evaluation_id', 'decision_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e101b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print method results \n",
    "for k, v in dict.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac5cc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp_cols_max_diff(df1: pd.DataFrame, df2: pd.DataFrame, cols: list, join_cols: list): \n",
    "    \"\"\"\n",
    "    Method will calculate the absolute difference in seconds for datetime64[ns] data type columns ,\n",
    "    over 2 pandas dataframes for rows of matching columns specified \n",
    "    and will return a dictionary object with this max difference \n",
    "    :param df1:The first pandas dataframe we want to compare with another \n",
    "    :param df2:The second pandas dataframe we want to compare with the first \n",
    "    :param cols: A list of datetime64[ns] type columns that we want to calculate their difference in seconds \n",
    "    :param join_cols: A list of columns to join df1 and df2\n",
    "    :return: A dictionary with key= the columns in cols and value=the max difference found in seconds between \n",
    "            rows of the 2 datasets\n",
    "    \"\"\"\n",
    "    error_cols=[]\n",
    "    for col in cols: \n",
    "        if not (is_datetime64_ns_dtype(df1[col]) and  is_datetime64_ns_dtype(df2[col])):\n",
    "            error_cols.append(col)\n",
    "    if error_cols:\n",
    "        print(f\"Columns are not datetime64-ns data type : {error_cols}\")\n",
    "    else:\n",
    "        print(\"All columns have the correct data type\")\n",
    "    m = pd.merge(df1, df2, how='inner', on= join_cols)\n",
    "    dict={}\n",
    "    for col in cols: \n",
    "        m[col+'_diff']=abs(m[col+'_x'] - m[col+'_y'])\n",
    "        dict[col+'_max_diff']=max(m[col+'_diff'])\n",
    "    return dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dbe61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gg=timestamp_cols_max_diff(rs1,mysql1,['claim_created_on', 'claim_last_modified_on','reason_created_on'], ['claim_number','claim_reason_id', 'claim_evaluation_id', 'decision_id'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e67db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print method results \n",
    "for k, v in gg.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a05274",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdc839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_int_cols_max_diff(df1: pd.DataFrame, df2: pd.DataFrame, cols: list, join_cols: list): \n",
    "    \"\"\"\n",
    "    Method will calculate the absolute difference between int64 or float64 data type columns ,\n",
    "    over 2 pandas dataframes for rows of matching columns specified \n",
    "    and will return a dictionary object with this max difference \n",
    "    :param df1:The first pandas dataframe we want to compare with another \n",
    "    :param df2:The second pandas dataframe we want to compare with the first \n",
    "    :param cols: A list of int64 or float64 type columns that we want to calculate their difference\n",
    "    :param join_cols: A list of columns to join df1 and df2\n",
    "    :return: A dictionary with key= the columns in cols and value=the max difference found in seconds between \n",
    "            rows of the 2 datasets\n",
    "    \"\"\"\n",
    "    error_cols=[]\n",
    "    for col in cols: \n",
    "        if not (is_numeric_dtype(df1[col]) and  is_numeric_dtype(df2[col])):\n",
    "            error_cols.append(col)\n",
    "    if error_cols:\n",
    "        print(f\"Columns are not float64 or int64 data type : {error_cols}\")\n",
    "    else:\n",
    "        print(\"All columns have the correct data type\")\n",
    "    m = pd.merge(df1, df2, how='inner', on= join_cols)\n",
    "    dict={}\n",
    "    for col in cols: \n",
    "        m[col+'_diff']=abs(m[col+'_x'] - m[col+'_y'])\n",
    "        dict[col+'_max_diff']=max(m[col+'_diff'])\n",
    "    return dict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fefc180",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff=float_int_cols_max_diff(rs1,mysql1,['car_mileage','evaluation_amount'],['claim_number','claim_reason_id', 'claim_evaluation_id', 'decision_id'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ae11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print method results \n",
    "for k, v in ff.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468ea83f",
   "metadata": {},
   "source": [
    "Extract dataset from comparison object of interest results locally for more drill down \n",
    "For example here  say that we are interested in column claim_created_on , so this will reside in comp[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8376aadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(comp[35].intersect_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6113f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('comparison_object.csv', sep=';', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
